{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import re\n",
    "import datetime as dt\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import tokenize\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import model_selection\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc,precision_score, accuracy_score, recall_score, f1_score\n",
    "from scipy import interp\n",
    "\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "import bokeh.plotting as bplt\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import logging\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, SimpleRNN, LSTM, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NigdyNieBędzieszSzłaSama_27_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\NigdyNieBędzieszSzłaSama_27_02.csv')\n",
    "# aborcjabezgranic_27_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\aborcjabezgranic_27_02.csv')\n",
    "# tojestwojna_27_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\tojestwojna_27_02.csv')\n",
    "# legalnaaborcja_27_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\legalnaaborcja_27_02.csv')\n",
    "# pieklokobiet_27_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\pieklokobiet_27_02.csv')\n",
    "# czarnyprotest_27_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\czarnyprotest_27_02.csv')\n",
    "# niedlazaostrzeniaustawyaborcyjnej_27_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\niedlazaostrzeniaustawyaborcyjnej_27_02.csv')\n",
    "# prawakobiet_27_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\prawakobiet_27_02.csv')\n",
    "# strajkkobiet_27_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\strajkkobiet_27_02.csv')\n",
    "\n",
    "# NigdyNieBędzieszSzłaSama28_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\#NigdyNieBędzieszSzłaSama28_02.csv')\n",
    "# aborcjabezgranic28_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\aborcjabezgranic28_02.csv')\n",
    "# tojestwojna28_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\tojestwojna28_02.csv')\n",
    "# legalnaaborcja28_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\legalnaaborcja28_02.csv')\n",
    "# pieklokobiet28_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\pieklokobiet28_02.csv')\n",
    "# czarnyprotest28_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\czarnyprotest28_02.csv')\n",
    "# niedlazaostrzeniaustawyaborcyjnej28_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\niedlazaostrzeniaustawyaborcyjnej28_02.csv')\n",
    "# prawakobiet28_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\prawakobiet28_02.csv')\n",
    "# strajkkobiet28_02=pd.read_csv('c:\\\\\\\\users\\\\\\\\hp\\\\\\\\downloads\\\\\\\\magisterka\\\\\\\\strajkkobiet28_02.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tweets = pd.concat([NigdyNieBędzieszSzłaSama_27_02,\n",
    "# aborcjabezgranic_27_02,\n",
    "# tojestwojna_27_02,\n",
    "# legalnaaborcja_27_02,\n",
    "# pieklokobiet_27_02,\n",
    "# czarnyprotest_27_02,\n",
    "# niedlazaostrzeniaustawyaborcyjnej_27_02,\n",
    "# prawakobiet_27_02,\n",
    "# strajkkobiet_27_02\n",
    "# ,NigdyNieBędzieszSzłaSama28_02,\n",
    "# aborcjabezgranic28_02,\n",
    "# tojestwojna28_02,\n",
    "# legalnaaborcja28_02,\n",
    "# pieklokobiet28_02,\n",
    "# czarnyprotest28_02,\n",
    "# #niedlazaostrzeniaustawyaborcyjnej28_02,\n",
    "# prawakobiet28_02,\n",
    "# strajkkobiet28_02])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import glob\n",
    "import pandas as pd\n",
    "# get data file names\n",
    "import os\n",
    "\n",
    "import os\n",
    "cwd = os.getcwd()\n",
    "\n",
    "print(cwd)\n",
    "\n",
    "path ='C:\\\\Users\\\\hp\\\\.spyder-py3\\\\PULPIT\\\\ALL_STRAJK'\n",
    "all_files  = glob.glob(path + \"\\\\*.csv\")\n",
    "\n",
    "path2 ='C:\\\\Users\\\\hp\\\\.spyder-py3\\\\PULPIT\\\\ALL_STRAJK2'\n",
    "all_files2  = glob.glob(path2 + \"\\\\*.csv\")\n",
    "    \n",
    "print(all_files)\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "#     print(filename)\n",
    "    df = pd.read_csv(filename, index_col=None, header=0,low_memory=False)\n",
    "    #print(df.head(5))\n",
    "    li.append(df)\n",
    "\n",
    "# for filename in all_files2:\n",
    "#    # print(filename)\n",
    "#     df = pd.read_csv(filename, index_col=None, header=0,low_memory=False)\n",
    "#     #print(df.head(5))\n",
    "#     li.append(df)    \n",
    "    \n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "\n",
    "tweets =frame.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = f\"{tweets[tweets.username =='strajkkobiet'].user_id[15857]:.9f}\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASHTAGI =['piekłokobiet',\n",
    "'wypierdalać',\n",
    "'wyroknakobiety',\n",
    "'falasprzeciwu',\n",
    "'strajkobiet',\n",
    "'zakazaborcji',\n",
    "'protestkobiet',\n",
    "'aborcja',\n",
    "'jebacpis',\n",
    "'jebaćpis',\n",
    "'osiemgwiazd',\n",
    "'osiemgwiazdek','wypierdalac','strajkkobiet',\n",
    "'piekłokobiet',\n",
    "'wypierdalać',\n",
    "'tojestwojna',\n",
    "'wyroknakobiety',\n",
    "'czarnyprotest',\n",
    "'falasprzeciwu',\n",
    "'strajkobiet',\n",
    "'pieklokobiet',\n",
    "'zakazaborcji',\n",
    "'prawakobiet',\n",
    "'protestkobiet',\n",
    "'aborcja',\n",
    "'jebacpis',\n",
    "'aborcjabezgranic',\n",
    "'piekłokobiet',\n",
    "'wypierdalac',\n",
    "'wyroknakobiety',\n",
    "'falasprzeciwu',\n",
    "'strajkobiet',\n",
    "'zakazaborcji',\n",
    "'protestkobiet',\n",
    "'aborcja',\n",
    "'jebacpis',\n",
    "'jebaćpis',\n",
    "'osiemgwiazd',\n",
    "'osiemgwiazdek',\n",
    "'wypierdalać',\n",
    "'wyroknakobiety',\n",
    "'falasprzeciwu',\n",
    "'strajkobiet',\n",
    "'zakazaborcji',\n",
    "'protestkobiet',\n",
    "'aborcja',\n",
    "'jebacpis',\n",
    "'jebaćpis',\n",
    "'osiemgwiazd',\n",
    "'osiemgwiazdek',\n",
    "'wypierdalac',\n",
    "'wypierdalac',\n",
    "'pieklokobiet',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['hashtags'] = tweets['hashtags'].map(str)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tweets['hashtags'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HASHTAGI2 =[piekłokobiet,\n",
    "# wypierdalać,\n",
    "# wyroknakobiety,\n",
    "# falasprzeciwu,\n",
    "# strajkobiet,\n",
    "# zakazaborcji,\n",
    "# protestkobiet,\n",
    "# aborcja,\n",
    "# jebacpis,\n",
    "# jebaćpis,\n",
    "# osiemgwiazd,\n",
    "# osiemgwiazdek,wypierdalac,strajkkobiet,\n",
    "# piekłokobiet,\n",
    "# wypierdalać,\n",
    "# tojestwojna,\n",
    "# wyroknakobiety,\n",
    "# czarnyprotest,\n",
    "# falasprzeciwu,\n",
    "# strajkobiet,\n",
    "# pieklokobiet,\n",
    "# zakazaborcji,\n",
    "# prawakobiet,\n",
    "# protestkobiet,\n",
    "# aborcja,\n",
    "# jebacpis,\n",
    "# aborcjabezgranic,\n",
    "# piekłokobiet,\n",
    "# wypierdalac,\n",
    "# wyroknakobiety,\n",
    "# falasprzeciwu,\n",
    "# strajkobiet,\n",
    "# zakazaborcji,\n",
    "# protestkobiet,\n",
    "# aborcja,\n",
    "# jebacpis,\n",
    "# jebaćpis,\n",
    "# osiemgwiazd,\n",
    "# osiemgwiazdek,\n",
    "# wypierdalać,\n",
    "# wyroknakobiety,\n",
    "# falasprzeciwu,\n",
    "# strajkobiet,\n",
    "# zakazaborcji,\n",
    "# protestkobiet,\n",
    "# aborcja,\n",
    "# jebacpis,\n",
    "# jebaćpis,\n",
    "# osiemgwiazd,\n",
    "# osiemgwiazdek,\n",
    "# wypierdalac,\n",
    "# wypierdalac,\n",
    "# pieklokobiet,\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = tweets[tweets['isHashtag'] ==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets = tweets[~tweets.duplicated()]\n",
    "tweets =tweets.drop_duplicates()\n",
    "tweets = tweets.loc[:,~tweets.columns.duplicated()]\n",
    "print(len(tweets)) # 20803 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets = tweets.head(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets.username=='martalempart'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets['date'] = pd.to_datetime(tweets['date'])\n",
    "tweets['hour'] = tweets['date'].apply(lambda x: x.hour)\n",
    "tweets['month'] = tweets['date'].apply(lambda x: x.month)\n",
    "tweets['day'] = tweets['date'].apply(lambda x: x.day)\n",
    "tweets['year'] = tweets['date'].apply(lambda x: x.year)\n",
    "tweets[\"tweet\"] = tweets[\"tweet\"].astype(str)\n",
    "tweets['length'] = tweets[\"tweet\"].apply(len)\n",
    "tweets['num_of_words'] = tweets[\"tweet\"].str.split().apply(len)\n",
    "\n",
    "tweets['dummy_count'] = 1\n",
    "tweets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets['time_decoded'] = pd.to_datetime(tweets.date)\n",
    "tweets['time_decoded'] = pd.to_datetime(tweets['time_decoded'], errors='coerce').dt.strftime('%Y-%m-%d')\n",
    "tweets[['date', 'time_decoded']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_hashtags=tweets[['id','hashtags']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets_hashtags.hashtags=tweets_hashtags.hashtags.map(lambda x: x.replace(\"[\", '')).map(lambda x: x.replace(\"]\", '')).map(lambda x: x.replace(\",\", ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_hashtags=tweets_hashtags.hashtags.str.split(expand=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_hashtags.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_hashtags['hash']=tweets_hashtags[0].append(tweets_hashtags[1]).append(tweets_hashtags[2]).append(tweets_hashtags[3]).append(tweets_hashtags[4]).append(tweets_hashtags[5]).append(tweets_hashtags[6]).append(tweets_hashtags[7]).append(tweets_hashtags[8]).append(tweets_hashtags[9]).append(tweets_hashtags[10]).append(tweets_hashtags[11]).append(tweets_hashtags[12]).append(tweets_hashtags[13]).append(tweets_hashtags[14]).append(tweets_hashtags[15]).append(tweets_hashtags[16]).append(tweets_hashtags[17]).append(tweets_hashtags[18]).append(tweets_hashtags[19]).append(tweets_hashtags[20]).append(tweets_hashtags[21]).append(tweets_hashtags[22]).append(tweets_hashtags[23]).append(tweets_hashtags[24]).append(tweets_hashtags[25]).append(tweets_hashtags[26]).append(tweets_hashtags[27]).append(tweets_hashtags[28]).append(tweets_hashtags[29]).append(tweets_hashtags[30]).append(tweets_hashtags[31]).append(tweets_hashtags[32]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_hashtags['hash2']=tweets_hashtags[0].append(tweets_hashtags[1]).append(tweets_hashtags[2]).append(tweets_hashtags[3]).append(tweets_hashtags[4]).append(tweets_hashtags[5]).append(tweets_hashtags[6]).append(tweets_hashtags[7]).append(tweets_hashtags[8]).append(tweets_hashtags[9]).append(tweets_hashtags[10]).append(tweets_hashtags[11]).append(tweets_hashtags[12]).append(tweets_hashtags[13]).append(tweets_hashtags[14]).append(tweets_hashtags[15]).append(tweets_hashtags[16]).append(tweets_hashtags[17]).append(tweets_hashtags[18]).append(tweets_hashtags[19]).append(tweets_hashtags[20]).append(tweets_hashtags[21]).append(tweets_hashtags[22]).append(tweets_hashtags[23]).append(tweets_hashtags[24]).append(tweets_hashtags[25]).append(tweets_hashtags[26]).append(tweets_hashtags[27]).append(tweets_hashtags[28]).append(tweets_hashtags[29]).append(tweets_hashtags[30]).append(tweets_hashtags[31]).append(tweets_hashtags[32]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_hashtags.head()\n",
    "grouped1 = pd.DataFrame(tweets_hashtags.groupby('hash').size().rename('counts')).sort_values('counts', ascending=False)\n",
    "grouped1.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tweets[tweets['time_decoded']<'2021-03-02']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets.isna().sum()\n",
    "tweets.groupby('timezone').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tweets['time_decoded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.sort_values('retweets_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grouped = pd.DataFrame(tweets.groupby('username').size().rename('counts')).sort_values('counts', ascending=False)\n",
    "grouped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().magic('matplotlib inline')\n",
    "tweets_by_username = tweets['username'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "ax.set_xlabel('Username', fontsize=12)\n",
    "ax.set_ylabel('Number of tweets' , fontsize=12)\n",
    "ax.set_title('Top 5 usernames twitting', fontsize=12, fontweight='bold')\n",
    "tweets_by_username[:8].plot(ax=ax, kind='bar', color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets['monthYear'] = tweets['date'].apply(lambda x: str(x.year)+'-'+str(x.month)+'-1')\n",
    "tweets['monthYear'].unique()[0]\n",
    "tweets['monthYear'] = pd.to_datetime(tweets['monthYear'])\n",
    "\n",
    "tweets[['date', 'monthYear']].head()\n",
    "\n",
    "yrMonthly_tweets2 = tweets.groupby(['month','year']).size().unstack()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "plt.title('Number of tweets by months', fontsize=12, fontweight='bold')\n",
    "plt.bar(yrMonthly_tweets2.index, yrMonthly_tweets2.values[:,0])\n",
    "plt.bar(yrMonthly_tweets2.index+12, yrMonthly_tweets2.values[:,1], color='g')\n",
    "plt.xticks(list(yrMonthly_tweets2.index)+list(yrMonthly_tweets2.index+12)+list(yrMonthly_tweets2.index[:8]+24))\n",
    "plt.show() \n",
    "\n",
    "x = yrMonthly_tweets2.values[:,1]\n",
    "x = x[~np.isnan(x)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tweets during a day\n",
    "# (liczba tweetow dziennie)\n",
    "\n",
    "tweets['weekDay']= tweets['date'].map(lambda x: date.isocalendar(x)[2])\n",
    "daily_tweets = tweets.groupby(['weekDay']).size()#.unstack()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "plt.title('Number of tweets by days', fontsize=12, fontweight='bold')\n",
    "plt.bar(daily_tweets.index, daily_tweets.values)\n",
    "plt.xticks(daily_tweets.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets['time'] = pd.to_datetime(tweets['time'], errors='coerce')\n",
    "tweets['time'] = tweets.time.dt.hour\n",
    "\n",
    "\n",
    "tweets.time.plot(kind='hist', bins=24, figsize=(10,5))\n",
    "plt.suptitle('Distribution of interactions at each hour of the day', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "nr_of_tweets = tweets.groupby('time_decoded').size() \n",
    "plt.figure(figsize=(18,5))\n",
    "plt.plot(pd.to_datetime(nr_of_tweets.index), nr_of_tweets) \n",
    "official_dates = list(tweets['date']) \n",
    "official_dates_stamp = pd.to_datetime(official_dates) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def word_in_text(word, text):\n",
    "    word = word.lower()\n",
    "    text = text.lower()\n",
    "    match = re.search(word, text)\n",
    "    if match:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Next, we will add column to tweets DataFrame\n",
    "tweets['wybory'] = tweets['tweet'].apply(lambda tweet: word_in_text('wybory', tweet))\n",
    "print(tweets['wybory'].value_counts()[True]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in HASHTAGI:\n",
    "    print(j)\n",
    "    tweets[j] = tweets['hashtags'].apply(lambda tweet: word_in_text(j, tweet))\n",
    "    print(tweets[j].value_counts()[True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.aborcja = tweets.aborcja.astype(int)\n",
    "tweets.piekłokobiet = tweets.piekłokobiet.astype(int)\n",
    "tweets.wypierdalać = tweets.wypierdalać.astype(int)\n",
    "tweets.wyroknakobiety = tweets.wyroknakobiety.astype(int)\n",
    "tweets.falasprzeciwu = tweets.falasprzeciwu.astype(int)\n",
    "tweets.strajkobiet = tweets.strajkobiet.astype(int)\n",
    "tweets.zakazaborcji = tweets.zakazaborcji.astype(int)\n",
    "tweets.protestkobiet = tweets.protestkobiet.astype(int)\n",
    "tweets.jebacpis = tweets.jebacpis.astype(int)\n",
    "tweets.jebaćpis = tweets.jebaćpis.astype(int)\n",
    "tweets.osiemgwiazd = tweets.osiemgwiazd.astype(int)\n",
    "tweets.osiemgwiazdek = tweets.osiemgwiazdek.astype(int)\n",
    "tweets.wypierdalac = tweets.wypierdalac.astype(int)\n",
    "tweets.strajkkobiet = tweets.strajkkobiet.astype(int)\n",
    "tweets.tojestwojna = tweets.tojestwojna.astype(int)\n",
    "tweets.czarnyprotest = tweets.czarnyprotest.astype(int)\n",
    "tweets.pieklokobiet = tweets.pieklokobiet.astype(int)\n",
    "tweets.prawakobiet = tweets.prawakobiet.astype(int)\n",
    "tweets.aborcjabezgranic = tweets.aborcjabezgranic.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['SUMA_HASH']= tweets.aborcjabezgranic +tweets.prawakobiet +tweets.pieklokobiet +tweets.czarnyprotest +tweets.tojestwojna +tweets.strajkkobiet +tweets.wypierdalac +tweets.osiemgwiazdek +tweets.osiemgwiazd +tweets.jebaćpis +tweets.jebacpis +tweets.protestkobiet +tweets.zakazaborcji +tweets.strajkobiet +tweets.falasprzeciwu +tweets.wyroknakobiety +tweets.wypierdalać +tweets.piekłokobiet +tweets.aborcja \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets['SUMA_HASH']==0].hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = re.sub(r'<.+?>',' ',s)  \n",
    "    s = re.sub(r'\\{[~\\{^\\{]+?\\}',' ',s)  \n",
    "    s = re.sub(r'\\n',' ', s)\n",
    "    s = re.sub(r'\\\\',' ', s) \n",
    "    s = re.sub(r'[\\,\\.\\\"\\-\\']',' ',s) \n",
    "    s = re.sub(\"\\d+\", \"\", s) \n",
    "    s = re.sub('[!%@#$_]', '', s)\n",
    "    s = re.sub(r'#\\S+', ' ', s) \n",
    "    s = re.sub(r'\\s{2,}',' ', s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['cleaning'] = [cleaning(s) for s in tweets['tweet']]\n",
    "tweets['cleaning'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = pd.read_csv('C:\\\\Users\\\\hp\\\\Downloads\\\\MAGISTERKA\\\\polishstopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    try:\n",
    "        tokens_ = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "        \n",
    "        tokens = []\n",
    "        \n",
    "        for token_by_sent in tokens_:\n",
    "            tokens += token_by_sent\n",
    "        \n",
    "        tokens = list(filter(lambda t: t.lower() not in stopwords.values, tokens))\n",
    "        tokens = list(filter(lambda t: t.lower() not in ['the','https'], tokens))\n",
    "\n",
    "        tokens = list(filter(lambda t: t not in punctuation, tokens))                     \n",
    "        tokens = list(filter(lambda t: t not in [u\"'s\", u\"n't\", u\"...\", u\"''\", u'``', \n",
    "                                            u'\\u2014', u'\\u2026', u'\\u2013'], tokens))\n",
    "        \n",
    "        tokens = list(filter(lambda t: '/' not in t, tokens))      \n",
    "        \n",
    "        tokens = list(filter(lambda t: len(t)>2, tokens))\n",
    "\n",
    "        filtered_tokens = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if re.search('[a-zA-Z]', token):\n",
    "                filtered_tokens.append(token)\n",
    "\n",
    "        filtered_tokens = list(map(lambda token: token.lower(), filtered_tokens))\n",
    "\n",
    "        return filtered_tokens\n",
    "    \n",
    "    except Error as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.mentions = tweets.mentions.apply(pd.Series)\n",
    "tweets.cashtags = tweets.cashtags.apply(pd.Series)\n",
    "tweets.urls = tweets.urls.apply(pd.Series)\n",
    "tweets.photos = tweets.photos.apply(pd.Series)\n",
    "tweets.link = tweets.link.apply(pd.Series)\n",
    "tweets.reply_to = tweets.reply_to.apply(pd.Series)\n",
    "\n",
    "\n",
    "tweets.mentions = tweets.mentions.replace('[]','')\n",
    "tweets.cashtags = tweets.cashtags.replace('[]','')\n",
    "tweets.urls = tweets.urls.replace('[]','')\n",
    "tweets.photos = tweets.photos.replace('[]','')\n",
    "tweets.link = tweets.link.replace('[]','')\n",
    "tweets.reply_to = tweets.reply_to.replace('[]','')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tokens'] = tweets.cleaning.map(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets[['tweet', 'cleaning', 'tokens']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, tokens in zip(tweets['tweet'].head(5), tweets['tokens'].head(5)):\n",
    "    print('tweet :', text)\n",
    "    print('tokens:', tokens)\n",
    "    print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# https://github.com/MarcinKosinski/trigeR5/blob/master/dicts/polimorfologik-2.1.zip/polimorfologik-2.1.txt\n",
    "\n",
    "words = pd.read_csv('C:\\\\Users\\\\hp\\\\Downloads\\\\MAGISTERKA\\\\polimorfologik-2.1.txt', sep=';', header=None)\n",
    "words.columns\n",
    "words = words.iloc[:, [0,1]]\n",
    "words = words.set_index([1])\n",
    "\n",
    "\n",
    "\n",
    "dictionary = words.iloc[:].to_dict(orient='dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_to_stem(word):\n",
    "    if word in dictionary[0]:\n",
    "        output = dictionary[0][word]\n",
    "    else: output = word\n",
    "    return output\n",
    "\n",
    "word_to_stem('polskiego')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets['stem'] = tweets['tokens'].map(lambda a: [word_to_stem(w) for w in a])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[['username', 'tweet', 'tokens', 'stem']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetsbackup =tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keywords():\n",
    "    tokens = tweets['tokens']\n",
    "    tokens_all = []\n",
    "    for each_token in tokens:\n",
    "        tokens_all += each_token\n",
    "    counter = Counter(tokens_all)\n",
    "    return counter.most_common(30)\n",
    "\n",
    "keywords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_SLOWA =['strajkkobiet',\n",
    " 'wypierdalać', 'kobiet', 'aborcja', \n",
    " 'piekłokobiet'\n",
    " 'pis', \n",
    " 'strajk',\n",
    " 'tojestwojna', \n",
    " 'strajku',\n",
    " 'lewica',  \n",
    " 'wyroknakobiety',  \n",
    " 'strajkobiet',  \n",
    " 'kobiety',  \n",
    " 'jebaćpis'  ,\n",
    " 'martalempart',\n",
    " 'aborcji',\n",
    " 'pisorgpl', 'dzieci',\n",
    " 'poland', \n",
    " 'tvpinfo',\n",
    " 'wypierdalac',\n",
    " 'prawa', 'wiosnabiedronia',\n",
    " 'ludzi', 'jebacpis',\n",
    " 'czarnyprotest',\n",
    " 'prawo', 'polsce', \n",
    " 'aborcją', \n",
    " 'joankasw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "list1 = tweets['stem'].agg(lambda x: ','.join(map(str, x)))\n",
    "\n",
    "list1 = pd.DataFrame(list1).stack().str.split(\"[^\\w+]\").explode().tolist()\n",
    "list1 = pd.DataFrame(list1).stack().str.split(\"[^\\w+]\").explode().tolist()\n",
    "list2 = ' '.join(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wordcloud = WordCloud(max_font_size=40, relative_scaling=0.3).generate(list2[:100000])\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [' '.join(a) for a in tweets['stem']] \n",
    "\n",
    "with open('articles.txt', 'wb') as ap:\n",
    "    pickle.dump(X, ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [' '.join(a) for a in tweets['stem']] \n",
    "\n",
    "c = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b',min_df=3,max_df=0.5) \n",
    "dtm = c.fit(X)\n",
    "art = dtm.transform(X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(M,dtm,k):\n",
    "    words = np.array(dtm.get_feature_names())\n",
    "    return(np.array([words[np.squeeze(np.array(np.argsort(M[i,:].todense())))[-k:]] for i in range(M.shape[0])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words(art,dtm,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocab = c.get_feature_names()\n",
    "np.array(vocab[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "words_cv = [(w, i, art.getcol(i).sum()) for w, i in dtm.vocabulary_.items()]\n",
    "words_cv = sorted(words_cv, key=lambda x: -x[2])[0:99]\n",
    "words_cv[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [' '.join(a) for a in tweets['stem']] \n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=10, max_features=10000, tokenizer=tokenizer, ngram_range=(1, 1))\n",
    "vz = vectorizer.fit_transform(X)\n",
    "vz.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "vz.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = vectorizer.get_feature_names()\n",
    "words[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidfy = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "tfidfy = pd.DataFrame(columns=['tfidf']).from_dict(dict(tfidfy), orient='index')\n",
    "tfidfy.columns = ['tfidf']\n",
    "tfidfy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tfidf = [(w, i, vz.getcol(i).sum()) for w, i in vectorizer.vocabulary_.items()]\n",
    "words_tfidf = sorted(words_tfidf, key=lambda x: -x[2])[0:99]\n",
    "words_tfidf[:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfy.sort_values(by=['tfidf'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfy.sort_values(by=['tfidf'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=23, random_state=0)\n",
    "svd_tfidf = svd.fit_transform(vz)\n",
    "svd_tfidf.shape\n",
    "\n",
    "svd.explained_variance_ratio_\n",
    "print(svd_tfidf.shape)\n",
    "len(svd_tfidf[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "tsne_tfidf = tsne_model.fit_transform(svd_tfidf)\n",
    "\n",
    "tsne_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "output_notebook()\n",
    "\n",
    "plot_tfidf = bp.figure(plot_width=900, plot_height=700, title=\"tf-idf clustering of tweets\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,box_select\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "tfidf_df = pd.DataFrame(tsne_tfidf, columns=['x', 'y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colorizing each tweet according to the cluster it belongs to - using bokeh\n",
    "\n",
    "colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n",
    "\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n",
    "\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n",
    "\"#52697d\", \"#7d6d33\", \"#d27c88\", \"#36422b\", \"#b68f79\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['tweet'] = tweets['tweet']\n",
    "\n",
    "plot_tfidf.scatter(x='x', y='y', source=tfidf_df )\n",
    "\n",
    "plt.figure()\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"text\": \"@text\"}\n",
    "\n",
    "show(plot_tfidf, notebook_handle=True)\n",
    "push_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 5\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=num_clusters, init='k-means++', n_init=1, \n",
    "                         init_size=1000, batch_size=1000, verbose=False, max_iter=1000)\n",
    "kmeans = kmeans_model.fit(vz)\n",
    "kmeans_clusters = kmeans.predict(vz)\n",
    "kmeans_distances = kmeans.transform(vz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (i, desc) in enumerate(tweets.tweet):\n",
    "    if(i < 5):\n",
    "        print(\"Cluster \" + str(kmeans_clusters[i]) + \": \" + desc + \n",
    "              \"(distance: \" + str(kmeans_distances[i][kmeans_clusters[i]]) + \")\")\n",
    "        print('--------------------------')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for j in sorted_centroids[i, :10]:\n",
    "        print(' %s' % terms[j], end='')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_kmeans = tsne_model.fit_transform(kmeans_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap = np.array([\"#6d8dca\", \"#69de53\", \"#723bca\", \"#c3e14c\", \"#c84dc9\", \"#68af4e\", \"#6e6cd5\",\n",
    "\"#e3be38\", \"#4e2d7c\", \"#5fdfa8\", \"#d34690\", \"#3f6d31\", \"#d44427\", \"#7fcdd8\", \"#cb4053\", \"#5e9981\",\n",
    "\"#803a62\", \"#9b9e39\", \"#c88cca\", \"#e1c37b\", \"#34223b\", \"#bdd8a3\", \"#6e3326\", \"#cfbdce\", \"#d07d3c\",\n",
    "\"#52697d\", \"#7d6d33\", \"#d27c88\", \"#36422b\", \"#b68f79\"])\n",
    "\n",
    "plot_kmeans = bp.figure(plot_width=700, plot_height=600, title=\"KMeans clustering of tweets\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,ywheel_pan\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "kmeans_df = pd.DataFrame(tsne_kmeans, columns=['x', 'y'])\n",
    "kmeans_df['cluster'] = kmeans_clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    " tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_kmeans.scatter(x=kmeans_df.x, y=kmeans_df.y, \n",
    "                    color=colormap[kmeans_clusters])\n",
    "\n",
    "bplt.figure()\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"tweet\": \"tweet\", \"cluster\":\"cluster\"}\n",
    "show(plot_kmeans,notebook_handle=True)\n",
    "push_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "cvectorizer = CountVectorizer(min_df=3, max_features=10000, tokenizer=tokenizer, ngram_range=(1,1))\n",
    "\n",
    "cvz = cvectorizer.fit_transform(tweets['tweet'])\n",
    "\n",
    "n_topics = 10\n",
    "n_iter = 100\n",
    "lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "X_topics = lda_model.fit_transform(cvz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 8\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.topic_word_ \n",
    "vocab = cvectorizer.get_feature_names() \n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words))) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.components_ \n",
    "lda_model.loglikelihood() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_lda = tsne_model.fit_transform(X_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get the main topic for each tweet\n",
    "doc_topic = lda_model.doc_topic_\n",
    "lda_keys = []\n",
    "for i, tweet in enumerate(tweets['tweet']):\n",
    "    lda_keys += [doc_topic[i].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tsne_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lda = bp.figure(plot_width=700, plot_height=600, title=\"LDA topic visualization\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,xwheel_pan\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "lda_df = pd.DataFrame(tsne_lda, columns=['x','y'])\n",
    "lda_df['tweet'] = tweets['tweet']\n",
    "\n",
    "lda_df['topic'] = lda_keys\n",
    "lda_df['topic'] = lda_df['topic'].map(int)\n",
    "from bokeh.models import ColumnDataSource\n",
    "\n",
    "source2 = ColumnDataSource(data = dict(color =colormap[lda_keys], x = lda_df.x, y = lda_df.y))\n",
    "\n",
    "\n",
    "plot_lda.scatter(source=source2, x='x', y='y',color='color')\n",
    "bplt.figure()\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"text\":\"@text\", \"topic\":\"@topic\"}\n",
    "show(plot_lda,notebook_handle=True)\n",
    "push_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_df['len_docs'] = tweets['tokens'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareLDAData():\n",
    "    data = {\n",
    "        'vocab': vocab,\n",
    "        'doc_topic_dists': lda_model.doc_topic_,\n",
    "        'doc_lengths': list(lda_df['len_docs']),\n",
    "        'term_frequency':cvectorizer.vocabulary_,\n",
    "        'topic_term_dists': lda_model.components_\n",
    "    } \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldadata = prepareLDAData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = pyLDAvis.prepare(**ldadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(prepared_data,'pyldadavis.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "docs = [TaggedDocument(tknzr.tokenize(cleaning(tweets['tweet'])), [i]) for i, tweet in enumerate(tweets)]\n",
    "doc2vec_model = Doc2Vec(docs, size=100, window=8, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_model.most_similar(positive=['strajkkobiet'], negative=[\"Policyjni\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = [doc2vec_model.docvecs[i] for i, t in enumerate(tweets[:10000])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_d2v = tsne_model.fit_transform(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_d2v = bp.figure(plot_width=900, plot_height=700, title=\"Tweets (doc2vec)\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,xwheel_pan\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_d2v.scatter(x=tsne_d2v[:,0], y=tsne_d2v[:,1],\n",
    "                    source=bp.ColumnDataSource({\n",
    "                        \"tweet\": tweets['tweet'],\n",
    "                        \"processed\": tweets['cleaning']\n",
    "                    })) # tweets['cleaning']\n",
    "\n",
    "hover = plot_d2v.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"text\": \"@tweet (processed: \\\"@processed\\\")\"}\n",
    "show(plot_d2v, notebook_handle=True)\n",
    "push_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tweets['stem'])\n",
    "corpus = [dictionary.doc2bow(text) for text in tweets['stem']]\n",
    "mm = corpora.MmCorpus.serialize('corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.keys()[:10]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dictionary.values()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = LdaModel(corpus=corpus,id2word=dictionary,num_topics=10,alpha=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.save('lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = []\n",
    "for doc in corpus:\n",
    "    topics.append(ldamodel[doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = np.array([len(t) for t in topics])\n",
    "print(np.mean(lens))\n",
    "# the average document mentions 5.8 topics and\n",
    "\n",
    "print(np.mean(lens <= 9))\n",
    "# 77 percent of them mention 9 or fewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also ask what the most talked about topic in is. First collect some statistics on topic usage\n",
    "counts = np.zeros(100)\n",
    "for doc_top in topics:\n",
    "    for ti,_ in doc_top:\n",
    "        counts[ti] +=1\n",
    "words = ldamodel.show_topic(counts.argmax(), 64)\n",
    "words[0]\n",
    "\n",
    "#Alternatively, we can look at the least talked about topic:\n",
    "#words = ldamodel.show_topic(counts.argmin(), 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of words - Word Cloud\n",
    "wordcloud = WordCloud(max_font_size=40, relative_scaling=0.3).generate(str(words))\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.print_topics(3)\n",
    "#print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of printing topics\n",
    "\n",
    "for i in range(0, ldamodel.num_topics-1):\n",
    "    print(ldamodel.print_topic(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.save('topic.model')\n",
    "from gensim.models import LdaModel\n",
    "loading = LdaModel.load('topic.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all topics\n",
    "ldamodel.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show 1 topic\n",
    "ldatopics = ldamodel.show_topics(formatted=False)[0]\n",
    "ldatopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.get_topic_terms(topicid=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel.get_document_topics(corpus[7]) # text representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tweets['stem'])):\n",
    "\n",
    "    print(ldamodel.get_document_topics(corpus[i], minimum_probability=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing LDA model with pyLDAvis. The area of the circles represents the prevelance of the topic\n",
    "# The length of the bars on the right represents the membership of a term in a particular topic\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "tweets2 = pyLDAvis.gensim.prepare(ldamodel,corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(tweets,'LDA-gensim.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Backup for sentiment analysis\n",
    "# # list1 = []\n",
    "\n",
    "# # for a in tweets['stem']:\n",
    "# #     list1 +=a\n",
    "\n",
    "# #     list2 = ''\n",
    "# # for a in list1:    \n",
    "# #     list2 = list2+' '+a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating exemplary vocabularies for positive (pos) and negative (neg) terms in Polish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing from list1\n",
    "\n",
    "pos = 'C:\\\\Users\\\\hp\\\\Downloads\\\\MAGISTERKA\\\\positive_words_pl.txt'\n",
    "neg = 'C:\\\\Users\\\\hp\\\\Downloads\\\\MAGISTERKA\\\\negative_words_pl.txt'\n",
    "\n",
    "pos2 = open(pos).read().split()\n",
    "neg2 = open(neg).read().split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing from list1\n",
    "\n",
    "wulgaryzmy = 'C:\\\\Users\\\\hp\\\\Downloads\\\\MAGISTERKA\\\\wulgaryzmy.txt'\n",
    "\n",
    "\n",
    "wulg =pd.read_csv(wulgaryzmy, header= 0,\n",
    "                        encoding= 'unicode_escape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting optimism score basing on the lists above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count = 0; neg_count = 0\n",
    "for a in list1[:]:\n",
    "    if a in pos2:\n",
    "        pos_count +=1\n",
    "    elif a in neg2:\n",
    "        neg_count +=1\n",
    "    else:\n",
    "        pass\n",
    "print(pos_count, neg_count)\n",
    "print('Optimism score: {:.2f}'.format((pos_count-neg_count)/(pos_count+neg_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "wulg_count = 0; neu_count = 0\n",
    "for a in list1[:]:\n",
    "    if a in wulg:\n",
    "        wulg_count +=1\n",
    "    else:\n",
    "        neu_count +=1\n",
    "\n",
    "print(wulg_count, neu_count)\n",
    "print('Wulg %: {:.10f}'.format(((wulg_count)/(wulg_count+neu_count))))\n",
    "\n",
    "\n",
    "# def wulga(s):\n",
    "#     wulg_count = 0\n",
    "#     neu_count = 0\n",
    "#     for a in s:\n",
    "#     #for lst in my_list:\n",
    "#         if a in wulg:\n",
    "#             wulg_count+=1\n",
    "#         else:\n",
    "#             neu_count +=1\n",
    "\n",
    "#     score = ((wulg_count)/(wulg_count+neu_count))+0.0000001\n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentiment(s):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for a in s:\n",
    "    #for lst in my_list:\n",
    "        if a in pos2:\n",
    "            pos_count+=1\n",
    "        elif a in neg2:\n",
    "            neg_count += 1\n",
    "        else:\n",
    "            pass\n",
    "    #score = (pos_count-neg_count)/(pos_count+neg_count)\n",
    "    score = (pos_count-neg_count)/(pos_count+neg_count+0.0000001)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['sentiment'] = tweets['stem'].apply(lambda a: sentiment(a))\n",
    "tweets[['stem', 'sentiment']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "tweets['sentiment'] = tweets['sentiment'].apply(lambda x: round(float(x), 1))\n",
    "tweets[['stem', 'sentiment']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets=pd.read_csv('C:\\\\Users\\\\hp\\\\Desktop\\\\TWEETS_DATA_SENT.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wulga_proc(s):\n",
    "    wulg_count = 0\n",
    "    neu_count = 0\n",
    "    for a in s:\n",
    "        if a in wulg.values:\n",
    "            wulg_count+=1\n",
    "            print(a)\n",
    "        else:\n",
    "            neu_count +=1\n",
    "\n",
    "    score = ((wulg_count)/(wulg_count+neu_count+0.00000000000001))*100\n",
    "    return score\n",
    "\n",
    "def wulga(s):\n",
    "    wulg_count = 0\n",
    "    neu_count = 0\n",
    "    for a in s:\n",
    "        if a in wulg.values:\n",
    "            wulg_count+=1\n",
    "            print(a)\n",
    "        else:\n",
    "            neu_count +=1\n",
    "    return wulg_count\n",
    "\n",
    "wulgaryzmy = 'C:\\\\Users\\\\hp\\\\Downloads\\\\MAGISTERKA\\\\wulgaryzmy.txt'\n",
    "\n",
    "\n",
    "wulg =pd.read_csv(wulgaryzmy, header= 0,\n",
    "                        encoding= 'unicode_escape')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "wulg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['wulga'] = tweets['tokens'].apply(lambda a: wulga(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['wulga_proc']= tweets['tokens'].apply(lambda a: wulga_proc(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.wulga_proc.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.wulga.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASHTAGI =['piekłokobiet',\n",
    "'wypierdalać',\n",
    "'wyroknakobiety',\n",
    "'falasprzeciwu',\n",
    "'strajkobiet',\n",
    "'zakazaborcji',\n",
    "'protestkobiet',\n",
    "'aborcja',\n",
    "'jebacpis',\n",
    "'jebaćpis',\n",
    "'osiemgwiazd',\n",
    "'osiemgwiazdek','wypierdalac','strajkkobiet',\n",
    "'piekłokobiet',\n",
    "'wypierdalać',\n",
    "'tojestwojna',\n",
    "'wyroknakobiety',\n",
    "'czarnyprotest',\n",
    "'falasprzeciwu',\n",
    "'strajkobiet',\n",
    "'pieklokobiet',\n",
    "'zakazaborcji',\n",
    "'prawakobiet',\n",
    "'protestkobiet',\n",
    "'aborcja',\n",
    "'jebacpis',\n",
    "'aborcjabezgranic',\n",
    "'piekłokobiet',\n",
    "'wypierdalac',\n",
    "'wyroknakobiety',\n",
    "'falasprzeciwu',\n",
    "'strajkobiet',\n",
    "'zakazaborcji',\n",
    "'protestkobiet',\n",
    "'aborcja',\n",
    "'jebacpis',\n",
    "'jebaćpis',\n",
    "'osiemgwiazd',\n",
    "'osiemgwiazdek',\n",
    "'wypierdalać',\n",
    "'wyroknakobiety',\n",
    "'falasprzeciwu',\n",
    "'strajkobiet',\n",
    "'zakazaborcji',\n",
    "'protestkobiet',\n",
    "'aborcja',\n",
    "'jebacpis',\n",
    "'jebaćpis',\n",
    "'osiemgwiazd',\n",
    "'osiemgwiazdek',\n",
    "'wypierdalac',\n",
    "'wypierdalac',\n",
    "'pieklokobiet',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOP_tfidfy =['strajkkobiet', 'wypierdalać', 'kobieta', 'aborcja', 'strajk',\n",
    "       'piekłokobiet', 'tojestwojna', 'chcieć', 'pis', 'lewica', 'mieć',\n",
    "       'prawy', 'protest', 'dziecko', 'ludzie', 'wyroknakobiety',\n",
    "       'strajkobiet', 'żyć', 'martalempart', 'wiedzieć']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_in_tweet3(word, data):\n",
    "\n",
    "    contains_column = data['stem'].str.contains(word, case = False)    \n",
    "    return contains_column\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_SLOWA =['strajkkobiet',\n",
    " 'wypierdalać', 'kobiet', 'aborcja', \n",
    " 'piekłokobiet'\n",
    " 'pis', \n",
    " 'strajk',\n",
    " 'tojestwojna', \n",
    " 'strajku',\n",
    " 'lewica',  \n",
    " 'wyroknakobiety',  \n",
    " 'strajkobiet',  \n",
    " 'kobiety',  \n",
    " 'jebaćpis'  ,\n",
    " 'martalempart',\n",
    " 'aborcji',\n",
    " 'pisorgpl', 'dzieci',\n",
    " 'poland', \n",
    " 'tvpinfo',\n",
    " 'wypierdalac',\n",
    " 'prawa', 'wiosnabiedronia',\n",
    " 'ludzi', 'jebacpis',\n",
    " 'czarnyprotest',\n",
    " 'prawo', 'polsce', \n",
    " 'aborcją', \n",
    " 'joankasw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for j in TOP_SLOWA:\n",
    "    tweets[\"STEM\"+j] = check_word_in_tweet3(j, tweets)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_in_tweet(word, data):\n",
    "\n",
    "    contains_column = data['hashtags'].str.contains(word, case = False)\n",
    "    return contains_column\n",
    "\n",
    "def check_word_in_tweet2(word, data):\n",
    "\n",
    "    contains_column = data['tweet'].str.contains(word, case = False)\n",
    "    return contains_column\n",
    " \n",
    "def check_word_in_tweet3(word, data):\n",
    "    contains_column = data['stem'].str.contains(word, case = False)    \n",
    "    return contains_column\n",
    "\n",
    "\n",
    "for j in TOP_tfidfy:\n",
    "    tweets[\"HASHTAGS\"+j] = check_word_in_tweet(j, tweets)\n",
    "\n",
    "\n",
    "for j in HASHTAGI:\n",
    "    tweets[\"HASHTAGS\"+j] = check_word_in_tweet(j, tweets)\n",
    "    \n",
    "for j in TOP_SLOWA:\n",
    "    tweets[\"HASHTAGS\"+j] = check_word_in_tweet(j, tweets)   \n",
    "    \n",
    "for j in TOP_tfidfy:\n",
    "    tweets[\"TWEET\"+j] = check_word_in_tweet2(j, tweets)\n",
    "\n",
    "\n",
    "for j in HASHTAGI:\n",
    "    tweets[\"TWEET\"+j] = check_word_in_tweet2(j, tweets)\n",
    "    \n",
    "for j in TOP_SLOWA:\n",
    "    tweets[\"TWEET\"+j] = check_word_in_tweet2(j, tweets)    \n",
    "    \n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping tweets by months\n",
    "groupedMonths = tweets.groupby('monthYear')['stem'].sum()\n",
    "\n",
    "groupedMonths = pd.DataFrame(groupedMonths)\n",
    "#groupedMonths\n",
    "\n",
    "groupedMonths['sentiment'] = groupedMonths['stem'].apply(lambda a: sentiment(a))\n",
    "#groupedMonths[groupedMonths['sentiment']<=0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedMonthsSK = tweets[tweets.username =='strajkkobiet'].groupby('monthYear')['stem'].sum()\n",
    "\n",
    "groupedMonthsSK = pd.DataFrame(groupedMonthsSK)\n",
    "groupedMonthsSK['sentiment'] = groupedMonthsSK['stem'].apply(lambda a: sentiment(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Sentiment')\n",
    "\n",
    "plt.plot(groupedMonths.index, groupedMonths['sentiment'], color='g', lw=3)\n",
    "\n",
    "\n",
    "key_events = [(pd.to_datetime('22.10.2020'), '22.10.2020'),\n",
    "(pd.to_datetime('28.11.2020'), '28.11.2020'),\n",
    "(pd.to_datetime('27.01.2021'), '27.01.2021')\n",
    "]\n",
    "\n",
    "\n",
    "for event in key_events:\n",
    "    ax.axvline(event[0], color='black')\n",
    "    ax.text(event[0] + pd.Timedelta(1, 'm'), 0.000000000000000000000000000000000000000000003, event[1], rotation=0, size=9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping tweets by months\n",
    "# (tweety po miesiącach)\n",
    "groupedweek = tweets.groupby('')['stem'].sum()\n",
    "\n",
    "groupedweek = pd.DataFrame(groupedMonths)\n",
    "#groupedMonths\n",
    "\n",
    "groupedweek['sentiment'] = groupedweek['stem'].apply(lambda a: sentiment(a))\n",
    "#groupedMonths[groupedMonths['sentiment']<=0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Sentiment')\n",
    "\n",
    "plt.plot(groupedMonths.index, groupedMonths['sentiment'], color='g', lw=3,label='Sentiments of all tweets')\n",
    "plt.plot(groupedMonthsSK.index, groupedMonthsSK['sentiment'], color='r', lw=3,label='Sentiments of tweets of user: STRAJKKOBIET')\n",
    "\n",
    "key_events = [(pd.to_datetime('22.10.2020'), '22.10.2020'),\n",
    "(pd.to_datetime('28.11.2020'), '28.11.2020'),\n",
    "(pd.to_datetime('27.01.2021'), '27.01.2021')\n",
    "]\n",
    "\n",
    "\n",
    "for event in key_events:\n",
    "    ax.axvline(event[0], color='black')\n",
    "    ax.text(event[0] + pd.Timedelta(1, 'm'), 0.000000000000000000000000000000000000000000003, event[1], rotation=0, size=9)\n",
    "\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive tweets\n",
    "positive = tweets[tweets['sentiment']>0.0][['sentiment','tweet']]\n",
    "positive.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neutral tweets\n",
    "neutral = tweets[tweets['sentiment']==0][['sentiment','tweet']]\n",
    "neutral.shape # (11942, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more negative tweets\n",
    "negative = tweets[tweets['sentiment']<=0.0][['sentiment','tweet']]\n",
    "\n",
    "negative.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['created_at'] = tweets['created_at'].astype(str).str[0:19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "import string\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_polarity(s):\n",
    "    return TextBlob(s).sentiment.polarity\n",
    "\n",
    "tweets['polarity'] = tweets['tweet'].apply(get_polarity)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "tweets.set_index(pd.DatetimeIndex(tweets['created_at']), inplace=True)\n",
    "\n",
    "polarity_series = tweets[tweets['polarity']!=0].polarity\n",
    "\n",
    "polarity_series = polarity_series.resample('D').mean()\n",
    "\n",
    "polarity_series.dropna(inplace=True)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Sentiment')\n",
    "\n",
    "polarity_series.plot(ax=ax, color='Blue', label='Sentiment')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Sentiment')\n",
    "\n",
    "\n",
    "key_events = [(pd.to_datetime('22.10.2020'), '22.10.2020'),\n",
    "(pd.to_datetime('28.11.2020'), '28.11.2020'),\n",
    "(pd.to_datetime('27.01.2021'), '27.01.2021')\n",
    "]\n",
    "\n",
    "\n",
    "for event in key_events:\n",
    "    ax.axvline(event[0], color='black')\n",
    "    ax.text(event[0] + pd.Timedelta(1, 'm'), 0.000000000000000000000000000000000000000000003, event[1], rotation=0, size=9)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['created_at'] = pd.to_datetime(tweets['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[tweets['polarity'] <= -0.5, 'polarity2'] = '-0.5 or lower' \n",
    "\n",
    "tweets.loc[(tweets['polarity'] > -0.5) & (tweets['polarity'] <0), 'polarity2'] = 'higher than -0.5 and lower than 0' \n",
    "\n",
    "tweets.loc[(tweets['polarity'] ==0), 'polarity2'] = '0' \n",
    "       \n",
    "tweets.loc[(tweets['polarity'] < 0.5) &(tweets['polarity'] > 0), 'polarity2'] = 'higher than 0.1 and lower than 0.5' \n",
    "tweets.loc[tweets['polarity'] >= 0.5, 'polarity2'] = '0.5 or higher' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[tweets['polarity'] <= 0, 'polarity2'] = 'lower than 0' \n",
    "\n",
    "tweets.loc[(tweets['polarity'] ==0), 'polarity2'] = '0' \n",
    "       \n",
    "tweets.loc[tweets['polarity'] >= 0, 'polarity2'] = 'higher than 0'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wulga_proc(s):\n",
    "    wulg_count = 0\n",
    "    neu_count = 0\n",
    "    for a in s:\n",
    "        if a in wulg.values:\n",
    "            wulg_count+=1\n",
    "            print(a)\n",
    "        else:\n",
    "            neu_count +=1\n",
    "\n",
    "    score = ((wulg_count)/(wulg_count+neu_count+0.00000000000001))*100\n",
    "    return score\n",
    "\n",
    "def wulga(s):\n",
    "    wulg_count = 0\n",
    "    neu_count = 0\n",
    "    for a in s:\n",
    "        if a in wulg.values:\n",
    "            wulg_count+=1\n",
    "            print(a)\n",
    "        else:\n",
    "            neu_count +=1\n",
    "    return wulg_count\n",
    "\n",
    "wulgaryzmy = 'C:\\\\Users\\\\hp\\\\Downloads\\\\MAGISTERKA\\\\wulgaryzmy.txt'\n",
    "\n",
    "\n",
    "wulg =pd.read_csv(wulgaryzmy, header= 0,\n",
    "                        encoding= 'unicode_escape')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['COUNT']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedMonths['wulga'] = groupedMonths['stem'].apply(lambda a: wulga_proc(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedMonthsSK = tweets[tweets.username =='strajkkobiet'].groupby('monthYear')['stem'].sum()\n",
    "\n",
    "groupedMonthsSK = pd.DataFrame(groupedMonthsSK)\n",
    "groupedMonthsSK['wulga'] = groupedMonthsSK['stem'].apply(lambda a: wulga_proc(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Vulgarity')\n",
    "\n",
    "plt.plot(groupedMonths.index, groupedMonths['wulga'], color='g', lw=3,label='Vulgarity of all tweets')\n",
    "\n",
    "key_events = [(pd.to_datetime('22.10.2020'), '22.10.2020'),\n",
    "(pd.to_datetime('28.11.2020'), '28.11.2020'),\n",
    "(pd.to_datetime('27.01.2021'), '27.01.2021')\n",
    "]\n",
    "\n",
    "\n",
    "for event in key_events:\n",
    "    ax.axvline(event[0], color='black')\n",
    "    ax.text(event[0] + pd.Timedelta(1, 'm'), 0.000000000000000000000000000000000000000000003, event[1], rotation=0, size=9)\n",
    "\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Vulgarity')\n",
    "\n",
    "plt.plot(groupedMonths.index, groupedMonths['wulga'], color='g', lw=3,label='Vulgarity of all tweets')\n",
    "plt.plot(groupedMonthsSK.index, groupedMonthsSK['wulga'], color='r', lw=3,label='Vulgarity of SK')\n",
    "\n",
    "\n",
    "#WITCZAK\n",
    "\n",
    "key_events = [(pd.to_datetime('22.10.2020'), '22.10.2020'),\n",
    "(pd.to_datetime('28.11.2020'), '28.11.2020'),\n",
    "(pd.to_datetime('27.01.2021'), '27.01.2021')\n",
    "]\n",
    "\n",
    "\n",
    "for event in key_events:\n",
    "    ax.axvline(event[0], color='black')\n",
    "    ax.text(event[0] + pd.Timedelta(1, 'm'), 0.000000000000000000000000000000000000000000003, event[1], rotation=0, size=9)\n",
    "\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupedMonths['wulga2'] = groupedMonths['stem'].apply(lambda a: wulga(a))\n",
    "\n",
    "groupedMonthsSK['wulga2'] = groupedMonthsSK['stem'].apply(lambda a: wulga(a))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Vulgarity')\n",
    "\n",
    "plt.plot(groupedMonths.index, groupedMonths['wulga2'], color='g', lw=3,label='Vulgarity of all tweets')\n",
    "\n",
    "key_events = [(pd.to_datetime('22.10.2020'), '22.10.2020'),\n",
    "(pd.to_datetime('28.11.2020'), '28.11.2020'),\n",
    "(pd.to_datetime('27.01.2021'), '27.01.2021')\n",
    "]\n",
    "\n",
    "\n",
    "for event in key_events:\n",
    "    ax.axvline(event[0], color='black')\n",
    "    ax.text(event[0] + pd.Timedelta(1, 'm'), 0.000000000000000000000000000000000000000000003, event[1], rotation=0, size=9)\n",
    "\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Vulgarity')\n",
    "\n",
    "plt.plot(groupedMonthsSK.index, groupedMonthsSK['wulga2'], color='r', lw=3,label='Vulgarity of SK')\n",
    "\n",
    "\n",
    "key_events = [(pd.to_datetime('22.10.2020'), '22.10.2020'),\n",
    "(pd.to_datetime('28.11.2020'), '28.11.2020'),\n",
    "(pd.to_datetime('27.01.2021'), '27.01.2021')\n",
    "]\n",
    "\n",
    "\n",
    "for event in key_events:\n",
    "    ax.axvline(event[0], color='black')\n",
    "    ax.text(event[0] + pd.Timedelta(1, 'm'), 0.000000000000000000000000000000000000000000003, event[1], rotation=0, size=9)\n",
    "\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tweets[tweets['polarity']!=0].groupby([pd.Grouper(key='created_at', freq='D'), 'polarity2'] ).count().unstack().stack().reset_index()\n",
    "\n",
    "\n",
    "result = result.rename(columns=                             \n",
    "    { \"COUNT\": \"Num of tweets\",  \n",
    "      \"created_at\":\"Time\" })\n",
    "\n",
    "\n",
    "import plotly.express as px\n",
    "fig = px.line(result, x='Time',                      \n",
    "    y=\"Num of tweets\",            \n",
    "    color='polarity2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['stem'] = tweets['stem'].map(str)  \n",
    "for j in TOP_tfidfy:\n",
    "    tweets[\"STEM\"+j] = check_word_in_tweet3(j, tweets)\n",
    "\n",
    "\n",
    "for j in HASHTAGI:\n",
    "    tweets[\"STEM\"+j] = check_word_in_tweet3(j, tweets)\n",
    "    \n",
    "for j in TOP_SLOWA:\n",
    "    tweets[\"STEM\"+j] = check_word_in_tweet3(j, tweets)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wulgaryzmy = 'C:\\\\Users\\\\hp\\\\Downloads\\\\MAGISTERKA\\\\wulgaryzmy.txt'\n",
    "\n",
    "\n",
    "wulg =pd.read_csv(wulgaryzmy, header= 0,\n",
    "                        encoding= 'unicode_escape')\n",
    "\n",
    "wulg = wulg['xxx'].explode().tolist()\n",
    "\n",
    "wulg_count = 0; neu_count = 0\n",
    "list21 = pd.DataFrame(tweets['tweet']).stack().str.split(\"[^\\w+]\").explode().tolist()\n",
    "\n",
    "\n",
    "for a in list21[:]:\n",
    "    if a in wulg:\n",
    "        wulg_count +=1\n",
    "    else:\n",
    "        neu_count +=1\n",
    "\n",
    "print(wulg_count, neu_count)\n",
    "print('Wulg %: {:.10f}'.format(((wulg_count)/(wulg_count+neu_count))))\n",
    "\n",
    "\n",
    "def wulga1(s):\n",
    "    wulg_count = 0\n",
    "    neu_count = 0\n",
    "    for a in s:\n",
    "    #for lst in my_list:\n",
    "        if a in wulg:\n",
    "            wulg_count+=1\n",
    "        else:\n",
    "            neu_count +=1\n",
    "\n",
    "    score = ((wulg_count))\n",
    "    return score\n",
    "\n",
    "\n",
    "# The 'wulga' function has been applied to each \"stemmed\" text through all rows \n",
    "# Each resulting value is then put into the 'wulga' column that is created after the assignment \n",
    "# (dodanie nowej kolumny z sentymentem na \"stemowanych\" tweetach)\n",
    "\n",
    "tweets['wulga'] = tweets['tweet'].apply(lambda a: wulga1(a))\n",
    "\n",
    "# Rounding\n",
    "tweets['wulga'] = tweets['wulga'].apply(lambda x: round(float(x), 0))\n",
    "\n",
    "groupedMonths['wulga'] = groupedMonths['stem'].apply(lambda a: wulga1(a))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "#plt.plot(gus.data, gus.stopa, color='r', lw=3) # not a good comparison after all \n",
    "plt.plot(groupedMonths.index, groupedMonths['wulga'], color='g', lw=1)\n",
    "key_events = [(pd.to_datetime('22.10.2020'), '22.10.2020'),\n",
    "(pd.to_datetime('28.11.2020'), '28.11.2020'),\n",
    "(pd.to_datetime('27.01.2021'), '27.01.2021')\n",
    "]\n",
    "\n",
    "\n",
    "for event in key_events:\n",
    "    ax.axvline(event[0], color='black')\n",
    "    ax.text(event[0] + pd.Timedelta(1, 'm'), 0.000000000000000000000000000000000000000000003, event[1], rotation=0, size=9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wulgaryzmy = 'C:\\\\Users\\\\hp\\\\Downloads\\\\MAGISTERKA\\\\wulgaryzmy.txt'\n",
    "\n",
    "\n",
    "wulg =pd.read_csv(wulgaryzmy, header= 0,\n",
    "                        encoding= 'unicode_escape')\n",
    "\n",
    "wulg = wulg['xxx'].explode().tolist()\n",
    "\n",
    "\n",
    "\n",
    "def wulga1(s):\n",
    "    wulg_count = 0\n",
    "    neu_count = 0\n",
    "    for a in s:\n",
    "    #for lst in my_list:\n",
    "        if a in wulg:\n",
    "            wulg_count+=1\n",
    "        else:\n",
    "            neu_count +=1\n",
    "\n",
    "    score = ((wulg_count/(wulg_count+neu_count)))\n",
    "    return score\n",
    "\n",
    "\n",
    "groupedMonths['wulga'] = groupedMonths['stem'].apply(lambda a: wulga1(a))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "#plt.plot(gus.data, gus.stopa, color='r', lw=3) # not a good comparison after all \n",
    "plt.plot(groupedMonths.index, groupedMonths['wulga'], color='g', lw=1)\n",
    "\n",
    "\n",
    "\n",
    "key_events = [(pd.to_datetime('22.10.2020'), '22.10.2020'),\n",
    "(pd.to_datetime('28.11.2020'), '28.11.2020'),\n",
    "(pd.to_datetime('27.01.2021'), '27.01.2021')\n",
    "]\n",
    "\n",
    "\n",
    "for event in key_events:\n",
    "    ax.axvline(event[0], color='black')\n",
    "    ax.text(event[0] + pd.Timedelta(1, 'm'), 0.000000000000000000000000000000000000000000003, event[1], rotation=0, size=9)\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "groupedMonthsSK['sentiment'] = groupedMonthsSK['stem'].apply(lambda a: sentiment(a))\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_title('Sentiment')\n",
    "\n",
    "plt.plot(groupedMonths.index, groupedMonths['sentiment'], color='g', lw=3,label='Vulgarism of all tweets')\n",
    "plt.plot(groupedMonthsSK.index, groupedMonthsSK['sentiment'], color='r', lw=3,label='Vulgarism of tweets of user: STRAJKKOBIET')\n",
    "\n",
    "\n",
    "#WITCZAK\n",
    "\n",
    "key_events = [(pd.to_datetime('22.10.2020'), '22.10.2020'),\n",
    "(pd.to_datetime('28.11.2020'), '28.11.2020'),\n",
    "(pd.to_datetime('27.01.2021'), '27.01.2021')\n",
    "]\n",
    "\n",
    "\n",
    "for event in key_events:\n",
    "    ax.axvline(event[0], color='black')\n",
    "    ax.text(event[0] + pd.Timedelta(1, 'm'), 0.000000000000000000000000000000000000000000003, event[1], rotation=0, size=9)\n",
    "\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.HASHTAGSstrajkkobiet = tweets.HASHTAGSstrajkkobiet.astype(int)\n",
    "tweets.HASHTAGSwypierdalać = tweets.HASHTAGSwypierdalać.astype(int)\n",
    "tweets.HASHTAGSkobieta = tweets.HASHTAGSkobieta.astype(int)\n",
    "tweets.HASHTAGSaborcja = tweets.HASHTAGSaborcja.astype(int)\n",
    "tweets.HASHTAGSstrajk = tweets.HASHTAGSstrajk.astype(int)\n",
    "tweets.HASHTAGSpiekłokobiet = tweets.HASHTAGSpiekłokobiet.astype(int)\n",
    "tweets.HASHTAGStojestwojna = tweets.HASHTAGStojestwojna.astype(int)\n",
    "tweets.HASHTAGSchcieć = tweets.HASHTAGSchcieć.astype(int)\n",
    "tweets.HASHTAGSpis = tweets.HASHTAGSpis.astype(int)\n",
    "tweets.HASHTAGSlewica = tweets.HASHTAGSlewica.astype(int)\n",
    "tweets.HASHTAGSmieć = tweets.HASHTAGSmieć.astype(int)\n",
    "tweets.HASHTAGSprawy = tweets.HASHTAGSprawy.astype(int)\n",
    "tweets.HASHTAGSprotest = tweets.HASHTAGSprotest.astype(int)\n",
    "tweets.HASHTAGSdziecko = tweets.HASHTAGSdziecko.astype(int)\n",
    "tweets.HASHTAGSludzie = tweets.HASHTAGSludzie.astype(int)\n",
    "tweets.HASHTAGSwyroknakobiety = tweets.HASHTAGSwyroknakobiety.astype(int)\n",
    "tweets.HASHTAGSstrajkobiet = tweets.HASHTAGSstrajkobiet.astype(int)\n",
    "tweets.HASHTAGSżyć = tweets.HASHTAGSżyć.astype(int)\n",
    "tweets.HASHTAGSmartalempart = tweets.HASHTAGSmartalempart.astype(int)\n",
    "tweets.HASHTAGSwiedzieć = tweets.HASHTAGSwiedzieć.astype(int)\n",
    "tweets.HASHTAGSfalasprzeciwu = tweets.HASHTAGSfalasprzeciwu.astype(int)\n",
    "tweets.HASHTAGSzakazaborcji = tweets.HASHTAGSzakazaborcji.astype(int)\n",
    "tweets.HASHTAGSprotestkobiet = tweets.HASHTAGSprotestkobiet.astype(int)\n",
    "tweets.HASHTAGSjebacpis = tweets.HASHTAGSjebacpis.astype(int)\n",
    "tweets.HASHTAGSjebaćpis = tweets.HASHTAGSjebaćpis.astype(int)\n",
    "tweets.HASHTAGSosiemgwiazd = tweets.HASHTAGSosiemgwiazd.astype(int)\n",
    "tweets.HASHTAGSosiemgwiazdek = tweets.HASHTAGSosiemgwiazdek.astype(int)\n",
    "tweets.HASHTAGSwypierdalac = tweets.HASHTAGSwypierdalac.astype(int)\n",
    "tweets.HASHTAGSczarnyprotest = tweets.HASHTAGSczarnyprotest.astype(int)\n",
    "tweets.HASHTAGSpieklokobiet = tweets.HASHTAGSpieklokobiet.astype(int)\n",
    "tweets.HASHTAGSprawakobiet = tweets.HASHTAGSprawakobiet.astype(int)\n",
    "tweets.HASHTAGSaborcjabezgranic = tweets.HASHTAGSaborcjabezgranic.astype(int)\n",
    "tweets.HASHTAGSkobiet = tweets.HASHTAGSkobiet.astype(int)\n",
    "tweets.HASHTAGSpiekłokobietpis = tweets.HASHTAGSpiekłokobietpis.astype(int)\n",
    "tweets.HASHTAGSstrajku = tweets.HASHTAGSstrajku.astype(int)\n",
    "tweets.HASHTAGSkobiety = tweets.HASHTAGSkobiety.astype(int)\n",
    "tweets.HASHTAGSaborcji = tweets.HASHTAGSaborcji.astype(int)\n",
    "tweets.HASHTAGSpisorgpl = tweets.HASHTAGSpisorgpl.astype(int)\n",
    "tweets.HASHTAGSdzieci = tweets.HASHTAGSdzieci.astype(int)\n",
    "tweets.HASHTAGSpoland = tweets.HASHTAGSpoland.astype(int)\n",
    "tweets.HASHTAGStvpinfo = tweets.HASHTAGStvpinfo.astype(int)\n",
    "tweets.HASHTAGSprawa = tweets.HASHTAGSprawa.astype(int)\n",
    "tweets.HASHTAGSwiosnabiedronia = tweets.HASHTAGSwiosnabiedronia.astype(int)\n",
    "tweets.HASHTAGSludzi = tweets.HASHTAGSludzi.astype(int)\n",
    "tweets.HASHTAGSprawo = tweets.HASHTAGSprawo.astype(int)\n",
    "tweets.HASHTAGSpolsce = tweets.HASHTAGSpolsce.astype(int)\n",
    "tweets.HASHTAGSaborcją = tweets.HASHTAGSaborcją.astype(int)\n",
    "tweets.HASHTAGSjoankasw = tweets.HASHTAGSjoankasw.astype(int)\n",
    "tweets.TWEETstrajkkobiet = tweets.TWEETstrajkkobiet.astype(int)\n",
    "tweets.TWEETwypierdalać = tweets.TWEETwypierdalać.astype(int)\n",
    "tweets.TWEETkobieta = tweets.TWEETkobieta.astype(int)\n",
    "tweets.TWEETaborcja = tweets.TWEETaborcja.astype(int)\n",
    "tweets.TWEETstrajk = tweets.TWEETstrajk.astype(int)\n",
    "tweets.TWEETpiekłokobiet = tweets.TWEETpiekłokobiet.astype(int)\n",
    "tweets.TWEETtojestwojna = tweets.TWEETtojestwojna.astype(int)\n",
    "tweets.TWEETchcieć = tweets.TWEETchcieć.astype(int)\n",
    "tweets.TWEETpis = tweets.TWEETpis.astype(int)\n",
    "tweets.TWEETlewica = tweets.TWEETlewica.astype(int)\n",
    "tweets.TWEETmieć = tweets.TWEETmieć.astype(int)\n",
    "tweets.TWEETprawy = tweets.TWEETprawy.astype(int)\n",
    "tweets.TWEETprotest = tweets.TWEETprotest.astype(int)\n",
    "tweets.TWEETdziecko = tweets.TWEETdziecko.astype(int)\n",
    "tweets.TWEETludzie = tweets.TWEETludzie.astype(int)\n",
    "tweets.TWEETwyroknakobiety = tweets.TWEETwyroknakobiety.astype(int)\n",
    "tweets.TWEETstrajkobiet = tweets.TWEETstrajkobiet.astype(int)\n",
    "tweets.TWEETżyć = tweets.TWEETżyć.astype(int)\n",
    "tweets.TWEETmartalempart = tweets.TWEETmartalempart.astype(int)\n",
    "tweets.TWEETwiedzieć = tweets.TWEETwiedzieć.astype(int)\n",
    "tweets.TWEETfalasprzeciwu = tweets.TWEETfalasprzeciwu.astype(int)\n",
    "tweets.TWEETzakazaborcji = tweets.TWEETzakazaborcji.astype(int)\n",
    "tweets.TWEETprotestkobiet = tweets.TWEETprotestkobiet.astype(int)\n",
    "tweets.TWEETjebacpis = tweets.TWEETjebacpis.astype(int)\n",
    "tweets.TWEETjebaćpis = tweets.TWEETjebaćpis.astype(int)\n",
    "tweets.TWEETosiemgwiazd = tweets.TWEETosiemgwiazd.astype(int)\n",
    "tweets.TWEETosiemgwiazdek = tweets.TWEETosiemgwiazdek.astype(int)\n",
    "tweets.TWEETwypierdalac = tweets.TWEETwypierdalac.astype(int)\n",
    "tweets.TWEETczarnyprotest = tweets.TWEETczarnyprotest.astype(int)\n",
    "tweets.TWEETpieklokobiet = tweets.TWEETpieklokobiet.astype(int)\n",
    "tweets.TWEETprawakobiet = tweets.TWEETprawakobiet.astype(int)\n",
    "tweets.TWEETaborcjabezgranic = tweets.TWEETaborcjabezgranic.astype(int)\n",
    "tweets.TWEETkobiet = tweets.TWEETkobiet.astype(int)\n",
    "tweets.TWEETpiekłokobietpis = tweets.TWEETpiekłokobietpis.astype(int)\n",
    "tweets.TWEETstrajku = tweets.TWEETstrajku.astype(int)\n",
    "tweets.TWEETkobiety = tweets.TWEETkobiety.astype(int)\n",
    "tweets.TWEETaborcji = tweets.TWEETaborcji.astype(int)\n",
    "tweets.TWEETpisorgpl = tweets.TWEETpisorgpl.astype(int)\n",
    "tweets.TWEETdzieci = tweets.TWEETdzieci.astype(int)\n",
    "tweets.TWEETpoland = tweets.TWEETpoland.astype(int)\n",
    "tweets.TWEETtvpinfo = tweets.TWEETtvpinfo.astype(int)\n",
    "tweets.TWEETprawa = tweets.TWEETprawa.astype(int)\n",
    "tweets.TWEETwiosnabiedronia = tweets.TWEETwiosnabiedronia.astype(int)\n",
    "tweets.TWEETludzi = tweets.TWEETludzi.astype(int)\n",
    "tweets.TWEETprawo = tweets.TWEETprawo.astype(int)\n",
    "tweets.TWEETpolsce = tweets.TWEETpolsce.astype(int)\n",
    "tweets.TWEETaborcją = tweets.TWEETaborcją.astype(int)\n",
    "tweets.TWEETjoankasw = tweets.TWEETjoankasw.astype(int)\n",
    "tweets.STEMstrajkkobiet = tweets.STEMstrajkkobiet.astype(int)\n",
    "tweets.STEMwypierdalać = tweets.STEMwypierdalać.astype(int)\n",
    "tweets.STEMkobieta = tweets.STEMkobieta.astype(int)\n",
    "tweets.STEMaborcja = tweets.STEMaborcja.astype(int)\n",
    "tweets.STEMstrajk = tweets.STEMstrajk.astype(int)\n",
    "tweets.STEMpiekłokobiet = tweets.STEMpiekłokobiet.astype(int)\n",
    "tweets.STEMtojestwojna = tweets.STEMtojestwojna.astype(int)\n",
    "tweets.STEMchcieć = tweets.STEMchcieć.astype(int)\n",
    "tweets.STEMpis = tweets.STEMpis.astype(int)\n",
    "tweets.STEMlewica = tweets.STEMlewica.astype(int)\n",
    "tweets.STEMmieć = tweets.STEMmieć.astype(int)\n",
    "tweets.STEMprawy = tweets.STEMprawy.astype(int)\n",
    "tweets.STEMprotest = tweets.STEMprotest.astype(int)\n",
    "tweets.STEMdziecko = tweets.STEMdziecko.astype(int)\n",
    "tweets.STEMludzie = tweets.STEMludzie.astype(int)\n",
    "tweets.STEMwyroknakobiety = tweets.STEMwyroknakobiety.astype(int)\n",
    "tweets.STEMstrajkobiet = tweets.STEMstrajkobiet.astype(int)\n",
    "tweets.STEMżyć = tweets.STEMżyć.astype(int)\n",
    "tweets.STEMmartalempart = tweets.STEMmartalempart.astype(int)\n",
    "tweets.STEMwiedzieć = tweets.STEMwiedzieć.astype(int)\n",
    "tweets.STEMfalasprzeciwu = tweets.STEMfalasprzeciwu.astype(int)\n",
    "tweets.STEMzakazaborcji = tweets.STEMzakazaborcji.astype(int)\n",
    "tweets.STEMprotestkobiet = tweets.STEMprotestkobiet.astype(int)\n",
    "tweets.STEMjebacpis = tweets.STEMjebacpis.astype(int)\n",
    "tweets.STEMjebaćpis = tweets.STEMjebaćpis.astype(int)\n",
    "tweets.STEMosiemgwiazd = tweets.STEMosiemgwiazd.astype(int)\n",
    "tweets.STEMosiemgwiazdek = tweets.STEMosiemgwiazdek.astype(int)\n",
    "tweets.STEMwypierdalac = tweets.STEMwypierdalac.astype(int)\n",
    "tweets.STEMczarnyprotest = tweets.STEMczarnyprotest.astype(int)\n",
    "tweets.STEMpieklokobiet = tweets.STEMpieklokobiet.astype(int)\n",
    "tweets.STEMprawakobiet = tweets.STEMprawakobiet.astype(int)\n",
    "tweets.STEMaborcjabezgranic = tweets.STEMaborcjabezgranic.astype(int)\n",
    "tweets.STEMkobiet = tweets.STEMkobiet.astype(int)\n",
    "tweets.STEMpiekłokobietpis = tweets.STEMpiekłokobietpis.astype(int)\n",
    "tweets.STEMstrajku = tweets.STEMstrajku.astype(int)\n",
    "tweets.STEMkobiety = tweets.STEMkobiety.astype(int)\n",
    "tweets.STEMaborcji = tweets.STEMaborcji.astype(int)\n",
    "tweets.STEMpisorgpl = tweets.STEMpisorgpl.astype(int)\n",
    "tweets.STEMdzieci = tweets.STEMdzieci.astype(int)\n",
    "tweets.STEMpoland = tweets.STEMpoland.astype(int)\n",
    "tweets.STEMtvpinfo = tweets.STEMtvpinfo.astype(int)\n",
    "tweets.STEMprawa = tweets.STEMprawa.astype(int)\n",
    "tweets.STEMwiosnabiedronia = tweets.STEMwiosnabiedronia.astype(int)\n",
    "tweets.STEMludzi = tweets.STEMludzi.astype(int)\n",
    "tweets.STEMprawo = tweets.STEMprawo.astype(int)\n",
    "tweets.STEMpolsce = tweets.STEMpolsce.astype(int)\n",
    "tweets.STEMaborcją = tweets.STEMaborcją.astype(int)\n",
    "tweets.STEMjoankasw = tweets.STEMjoankasw.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweets.to_csv('C:\\\\Users\\\\hp\\\\Desktop\\\\TWEETS_DATA_SENT1_1.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
